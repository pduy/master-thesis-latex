\chapter{Methodology\label{cha:methodology}}
In this chapter, we describe how we design and implement the experiments with
\acrshort{gan}s to achieve the objectives of the thesis.

\section{Our \acrshort{gan}s }
The main contribution of the thesis is to use the concept of \acrshort{gan} to
adversarially train a generative model which can produce a reasonable depth-map of a
given 2D image of a particular object. The model is expected to be able to
understand the 3D properties of images of single objects. Because of its tasks, we call
this one the "Depth-GAN".

In addition, we also train another \acrshort{gan} to learn the 3D knowledge in another
dimension, the pose estimator. This \acrshort{gan}'s job is to generate a 2D image of the
same given object in another pose. It is expected to be able to rotate, in 3D space, the
object by a particular angle while being conditioned on the object 2D image and its
corresponding depth-map. We call this one the "Pose-GAN"

\subsection{Network Architecture}

We inherit the architecture from \acrshort{pix} with only some occasional modifications in
the input and output layers because our experiments, especially the first one which
generates depth-maps from an RGB image, is very close to their settings. The detailed
settings are shown in chapter \ref{cha:model}.

The Generator is basically an U-Net \cite{u_net}, an encoder-decoder with skip
connections between every decoder layer and the corresponding encoder layer. The intuition
behind this is that, there are relevant information between the input and the output.
Especially the mirrors in the encoder part and the decoder part share the same resolution
and same level of encoding. Therefore, letting them connect directly is a good idea.
Figure~\ref{fig:u_net} visualizes the architectures of both a standard Encoder-Decoder and
U-Net.

Similarly, the Discriminator is also a Convolution-Batch-Norm-ReLu structure. It
implements an idea which the authors call "PatchGAN", where an image are divided to some
$N \times N$ patches and the network only judges the real-fake properties of each patch.
The final decision is averaged over the single-patch decisions. The rest of its job are
exactly the same as a normal binary classifier.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\linewidth]{u_net}
	\caption{Standard Encoder-Decoder and U-Net architectures}
	\label{fig:u_net}
\end{figure}

\subsection{Training \acrshort{gan}s}
\label{sub:training_gan}

In training \acrshort{gan}s, the Discriminator Loss is usually much more important than
the Generator Loss because it can show if the adversarial training has converged or
not. As being already explained in Chapter \ref{cha:relatedwork}, when the (absolute) Discriminator
Loss is stable at 1.38629436112, we know that they two have reached an equilibrium. 

\acrshort{gan}s are notorious for being very hard to train. As it does not minimize a
simple scalar loss function, fluctuation in training is very popular and the network does
not converge eventually. This is a very hard problem and researchers are still working on
it. Fortunately, it does not always happens. Another common issue is the
Vanishing Gradient problem caused by a very quickly-converged Discriminator. In most of
the cases, the job of the Discriminator is much simpler than that of the Generator. When
its loss is too small, the gradient quickly vanishes in the Back-Propagation steps and
prevent the Generator to improve. The consequence is that the Discriminator Loss can never
reach the equilibrium of 1.38629436112.

A common method to solve this problem is to regularize the Discriminator. In this thesis, we
occasionally apply the following tricks:

\begin{itemize}
	\item Slowing down the Discriminator by reducing its learning rate or making it wait
		for two or even more update steps of the Generator
	\item Adding instance noise to the inputs of the Discriminator. \cite{kaae}
	\item Doing One-sided Label Smoothing: replacing the label of the real image
		by a random number in a small range around 1, [0.7, 1.2] for example, instead of 1
		\cite{saliman}
\end{itemize}

While the first two methods are quite trivial as they aim to make the Discriminator job
harder to balance the work with the Generator, the third method has an interesting
explanation in Salimans et al. \cite{saliman}. The authors reuse an old concept from
several decades ago, Label Smoothing, and state that it helps reduce the sensitivity of
the network with respect to adversarial examples. The reason why we smooth only the real
samples (label 1) is that, when we apply the trick we now have the new optimal
Discriminator $D(x) = \frac{\alpha p_{data}(x) + \beta p_{model}(x)}{p_{data}(x) +
p_{model}(x)}$. When $p_{data}(x)$ is very low and $p_{model}(x)$ is very large,
$p_{model}$ has no incentive to move towards $p_{data}$. Setting $\beta = 0$, as in usual
\acrshort{gan}s, completely removes $p_{model}$ from the equation, thus prevents the
problem.

Besides, in training our Pose-GAN, we also perform another trick: removing all of the
round-shape objects, such as "Apple", "Bowl" and "Food Can". We believe that those items can
confuse the \acrshort{gan} instead of adding important values. This idea is inferred from
Elhoseiny et al. \cite{elhoseiny}, where the authors also select only 34 out of 51
categories but do not declare explicitly which categories are chosen. In our
Pose-GAN, we came up with the same number of to-be-removed categories, which are listed in
Table \ref{tab:removed_objects}.

\begin{table}[h!]
	\centering
	\caption{The round-shape objects removed from Pose-GAN training set}
	\label{tab:removed_objects}
	\begin{tabular}{@{}llllll@{}}
		\toprule
		\textbf{Category}              & \textbf{Example}      & \textbf{Category}
		& \textbf{Example}      & \textbf{Category}             & \textbf{Example}      \\
		\midrule \multicolumn{1}{|l|}{Apple}    &
		\multicolumn{1}{l|}{\includegraphics[height=0.08\textwidth]{apple_1_1_1_crop}} &
		\multicolumn{1}{l|}{Garlic}       &
		\multicolumn{1}{l|}{\includegraphics[height=0.08\textwidth]{garlic_1_1_1_crop}} &
		\multicolumn{1}{l|}{Food Jar} &
		\multicolumn{1}{l|}{\includegraphics[height=0.08\textwidth]{food_jar_1_1_1_crop}}
		\\
		\midrule 
		\multicolumn{1}{|l|}{Peach}    &
		\multicolumn{1}{l|}{\includegraphics[height=0.08\textwidth]{peach_1_1_1_crop}} &
		\multicolumn{1}{l|}{Tomato}   &
		\multicolumn{1}{l|}{\includegraphics[height=0.08\textwidth]{tomato_1_1_1_crop}} &
		\multicolumn{1}{l|}{Food Cup} &
		\multicolumn{1}{l|}{\includegraphics[height=0.08\textwidth]{food_cup_1_1_1_crop}}
		\\
		\midrule 
		\multicolumn{1}{|l|}{Food Can} &
		\multicolumn{1}{l|}{\includegraphics[height=0.08\textwidth]{food_can_1_1_1_crop}} & 
		\multicolumn{1}{l|}{Lemon}        &
		\multicolumn{1}{l|}{\includegraphics[height=0.08\textwidth]{lemon_1_1_1_crop}} &
		\multicolumn{1}{l|}{Glue Stick}   &
		\multicolumn{1}{l|}{\includegraphics[height=0.08\textwidth]{glue_stick_1_1_1_crop}}
		\\
		\midrule 
		\multicolumn{1}{|l|}{Orange}   &
		\multicolumn{1}{l|}{\includegraphics[height=0.08\textwidth]{orange_1_1_1_crop}} &
		\multicolumn{1}{l|}{Lime}         &
		\multicolumn{1}{l|}{\includegraphics[height=0.08\textwidth]{lime_1_1_1_crop}} &
		\multicolumn{1}{l|}{Water Bottle} &
		\multicolumn{1}{l|}{\includegraphics[height=0.08\textwidth]{water_bottle_1_1_1_crop}}
		\\
		\midrule 
		\multicolumn{1}{|l|}{Onion}    &
		\multicolumn{1}{l|}{\includegraphics[height=0.08\textwidth]{onion_1_1_1_crop}} &
		\multicolumn{1}{l|}{Mushroom}     &
		\multicolumn{1}{l|}{\includegraphics[height=0.08\textwidth]{mushroom_1_1_1_crop}} &
		\multicolumn{1}{l|}{Soda Can} &
		\multicolumn{1}{l|}{\includegraphics[height=0.08\textwidth]{soda_can_1_1_1_crop}} \\
		\midrule 
		\multicolumn{1}{|l|}{Bowl}     &
		\multicolumn{1}{l|}{\includegraphics[height=0.08\textwidth]{bowl_1_1_1_crop}} &
		\multicolumn{1}{l|}{Plate}    &
		\multicolumn{1}{l|}{\includegraphics[height=0.08\textwidth]{plate_1_1_1_crop}} &

		&                       \\ \cmidrule(r){1-4}
	\end{tabular}
\end{table}

\subsection{Data Manipulation}

As being mentioned above, in this thesis we focus on 2 tasks on \acrshort{gan}: learning
to generate depth information and learning to generate another pose of an object. Just to
clarify the terminologies, as we are doing image-to-image translation tasks, we call the
conditional image the "input", the to-be-generated image the "output", and the
ground-truth the "target". In that way, the Generator is supposed to make the "output" as
close to the "target" as possible, while the Discriminator still tries its best to tell
them apart.

In the first \acrshort{gan}, we line the Washington RGB-D dataset up by putting every RGB
image along with the corresponding depth-map. Here the RGB is the input, the generated
depth-map is the output, and the depth-map provided by the RGB-D dataset is the target. As
the RGB image is an 8-bit one and the depth-map is 16-bit, we have to manipulate the data
loading process to make sure that we have the correct forms. A wrong manipulation can lead
to a great loss of information.

In the additional \acrshort{gan} where we want to learn to generate a different pose of
the same object, we use both the RGB frame and the corresponding (real) depth-map as the
input condition to \acrshort{gan}, to both the Generator and Discriminator. The depth-map
is pre-processed and attached to the RGB frame as the 4th dimension, along with the three
color channels. This \acrshort{gan} will then use this information to produce a normal
3-channel RGB image of the same object, but in another pose. The Discriminator's task is
to classify this rotated image with the real image in that pose. Here the input is a
4-dimensional image, the output is an RGB, and the target is a real RGB image of that
pose.

For every frame, we choose the target pose of the 10th frame in the video sequence, which
is equivalent to a rotation of roughly 20\degree. We do that for several reasons. First,
there are subtle differences and errors among different camera scans, which makes choosing
an exact pose, such as 15\degree or 20\degree, unrealistic. Second, as we also have a
complex stratified Train-Test split which will be explained in details in section
\ref{sec:train_test_split}, we would like to make sure that the target pose of every input
frame should be in the same part of the split. A combination of an input from the training
split and a target from the test split can lead to dangerous bias in evaluation, for both
the \acrshort{gan} and the baseline classifier. Because we pick every 5th frame from every
instance to be in the test set, choosing the 10th frame as the target makes sure that the
rotated pose is in the same train-test split. Finally, a target rotation of
roughly 20\degree is a reasonable choice as it is big enough to observe a clear rotation
but still contains many similar details as the input, which makes the task more feasible.

For all types of \acrshort{gan}s that we are working with, the Generator produces the
Output, and the Discriminator reads both the Output and the Target and try its best to
classify the two, while both of them are conditioned on the Input. To simplify the
language, from this point we will sometimes refer both of them (i.e. the Generator and the
Discriminator) to \acrshort{gan} only. For instance, the statement "The \acrshort{gan} reads
image A and generates image B" is equivalent to "The Generator generates image B and the
Discriminator tries to distinguish image B and a target B', while both of them are
conditioned on image A"

\subsection{Handling depth missing values \label{sub:missing_values}}

Due to technical limitations, there are corners that the Kinect sensor cannot capture the
depth information. Those values are represented as 0's in the depth-maps. If we just keep
the depth-maps as they are and feed them to the Discriminator, the \acrshort{gan} will
learn the missing value patterns and will probably somehow generate a representation of
missing values according to its understanding. This is not really what we want; thus,
interpolating the missing values can be a good pre-processing step.

In this thesis, we use the interpolation method from a toolbox from the authors of NYU
Dataset \cite{nyu_dataset}, which applies the colorization method from Levin et al.
\cite{levin_colorization}. The NYU Dataset is another one with RGB-D Images recorded with
Kinect. Unlike the Washington Object Dataset, the NYU contains mostly large indoor scenes
instead of single objects. The colorization algorithm uses the RGB information to infer
the depth values of the missing points. The details of the optimization method can be
found in the paper \cite{levin_colorization}.

\section{Baseline task}

For the baseline task, we keep most of the architecture of the two-channel network proposed
by Eitel et al. Our implementation makes some changes on the manipulation of the depth
missing values and the way we train the network. 

First, regarding the depth missing values, in the paper, the authors propose a noise
injection method to the depth-maps in the training process. The idea is too mimic the
missing values patterns (the 0 values in the maps) so that the classifier becomes robust
towards them. In other words, missing values are considered as noise in data. In our
implementation, because we also remove those missing values from the \acrshort{gan}
training process for some reasons that we have explained in section
\ref{sub:missing_values}, we also do the same to the classification. Basically, we would
like the data we use to train our \acrshort{gan} and the classifier to have identical
formats.

Second, regarding the training strategy, as our approach of handling the missing values in
\ref{sub:missing_values} already gives better accuracies on the original data, we do not
perform fine-tuning the AlexNet channels as the authors did. A benefit of no fine-tuning
is that we can utilize the AlexNet representations for different training rounds without
re-doing back-propagation throughout the entire 9 layers of the two-channel network. In
fact, we only train the 2 last layers for most of the experiments.

For evaluating our Pose-GAN, we keep the same architecture, but instead of using the
colorized depth-map for the second channel, we feed the target pose of every frame. In
this way, for every item the network will receive a pair of RGB images in 2 different
pose, which we call "Stereo RGB Input". Our objective is to find out if training with 
stereo images is better than with single RGB image or not. Depth maps are not involve in
this experiment.

\section{Evaluation Method \label{sec:train_test_split}}
%\subsection{Train-Test split methodology\label{sec:train_test_split}}
As we are working on 2 Deep Learning tasks and the dataset contains many details in
different layers, a naive random Train-Test split is not the optimal approach. For this
purpose, we design a more stratified split, which is demonstrated in
Figure~\ref{fig:gan_train_test_split}.

\begin{figure}[h!]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=0.8\linewidth]{img/gan_train_test_split_depth}
		\caption{Evaluating Depth-GAN}
		\label{subfig:depth_gan_split}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=0.8\linewidth]{img/gan_train_test_split_pose}
		\caption{Evaluating Pose-GAN. Note the difference here: GAN
		generated data is combined with GAN Test Set instead of GAN Training Set.}
		\label{subfig:pose_gan_split}
	\end{subfigure}

	\caption{Our stratified Train-Test split for evaluating \acrshort{gan}s. The
	Stratified-Sampled Training set is split into a GAN Training Set and GAN Test Set with
different proportions.}
\label{fig:gan_train_test_split}
\end{figure}

For the baseline classifier, we split the data using the same strategy as in Eitel et al
\cite{eitel}, the work we refer to. In each instance, every fifth frame is sampled
and in every object, one instance is randomly and completely taken out. This gives us
about 138,000 items for training and about 69,000 items for testing.  We also perform the
10-cross-validation splits introduced in Lai et al \cite{washington_rgbd}. That is, each of the
lifting angle sequence is divided in to 3 sub-sequences (let us recall that each instance
in the Washington RGB-D dataset is recorded in 3 different lifting angles: 30, 45 and 60
degrees). Among those 9 sub-sequences, two are randomly added to the evaluation set. In
total, each experiment is done by using about 128,000 items for training and about 78,000
items for testing.

For the Depth-GAN, as we aim to use the \acrshort{gan} results to substitute
the real data in training the baseline classifier, we also have to design an additional
train-test split. The evaluation set generated from the previous strategy is left alone
without any further modification. Instead, we randomly split the classifier's training set
in different scales: 50-50, 25-75 and 10-90. The left portions are used for training the
\acrshort{gan} and the right portions are used to evaluate it, of which the results will
substitute the corresponding real items in the training set of the classifier. In this
way, we make sure that:

\begin{itemize}
	\item \acrshort{gan} has not seen the items it is generating samples from. In
		other words, \acrshort{gan} is in evaluation mode when being used for training the
		baseline classifier. This is important because we cannot directly use the training output
		from \acrshort{gan} to train the baseline classifier, which is bias and encourages over-fitting.
	\item We can objectively compare the classification results trained from similar
		amount of data but with different portions of real-fake components. In this way,
		we can see how \acrshort{gan} contributes to the classification accuracies in
		different scales.
\end{itemize}

However, this is worth knowing that this approach also has a drawback. The comparisons of
different \acrshort{gan} training setups are not entirely fair because the amounts of data
used in training the \acrshort{gan} are different. For instance, if we would like to have
90\% of \acrshort{gan} data to train the classifier, we can use only the remaining 10\% of
the data to train the \acrshort{gan} to make sure that the generated data has not been
seen by \acrshort{gan} before. However, if we keep only 50\% of \acrshort{gan} data in the
classifier's training set, we can use up to 50\% of the remaining data to train it, which
potentially results in a stronger \acrshort{gan}. 

For the Pose-GAN, the train-test split is mostly similar but has an important difference
from how we do with the Depth-GAN. After training the \acrshort{gan}, we combine the "GAN
Generated Data" with "GAN Test Set" instead of "GAN Training Set", demonstrated clearly in
Figure~\ref{subfig:pose_gan_split}. The reason is that our generated data now contains all
the poses supposedly in the GAN Training Set because we are doing pose rotation. In other
words, we are replacing the "GAN Training Set" by the "GAN Generated Data". Besides,
because of time limitation, we do not perform the experiments in different proportions like
what we do with the Depth-GAN; only the 50-50 proportion is experimented.

